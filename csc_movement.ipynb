{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfCBzvSnWV06z/5B0wJOn2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imj2185/csc_movement/blob/master/csc_movement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqYiGLBf9cPr",
        "outputId": "17210768-f654-44ad-e1fd-ca40ee56b4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.7-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import argparse\n",
        "import xlsxwriter"
      ],
      "metadata": {
        "id": "MydbtAIf99HK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_dtype(torch.float64)"
      ],
      "metadata": {
        "id": "POQMF4ZS-Avw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DEDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y, scale_data=False):\n",
        "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "            #Apply scaling if necessary\n",
        "            if scale_data:\n",
        "                X = StandardScaler().fit_transform(X)\n",
        "            self.X = torch.from_numpy(X)\n",
        "            self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]"
      ],
      "metadata": {
        "id": "UfAvXmfW-Fgp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(25, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "REqV47Yb-JCM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_number=1\n",
        "val_number=4\n",
        "val_count=51\n",
        "sample_name='Xylem10%'\n",
        "train=True\n",
        "test=False\n",
        "\n",
        "mlp = MLP()"
      ],
      "metadata": {
        "id": "SwR0MZP4-LZ2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "l2gC1TIq-qy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "    \n",
        "# Load dataset\n",
        "DE_list = ['DE0', 'DE50', 'DE100']\n",
        "xls = pd.ExcelFile('./train_' + str(exp_number)+ '/train.xlsx')\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for de in DE_list:\n",
        "    azimuth = []\n",
        "    df1 = pd.read_excel(xls, de)\n",
        "\n",
        "    tilt_angle_1 = [i*15 for i in range(12)]\n",
        "    tilt_angel_2 = [i*(-15) for i in range(1, 13)]\n",
        "\n",
        "    tilt_angle = tilt_angle_1 + tilt_angel_2\n",
        "\n",
        "    for i in tilt_angle:\n",
        "        azimuth.append(df1[i].values)\n",
        "\n",
        "    tilt_angel_2 = [(i*(-15) + 360) for i in range(1, 13)]\n",
        "    tilt_angle = tilt_angle_1 + tilt_angel_2\n",
        "\n",
        "    for i in range(len(tilt_angle)):\n",
        "        tilt_angle[i] = [((tilt_angle[i] * math.pi) / 180.0), float(de[2:]) / 100.0]\n",
        "\n",
        "    X = X + azimuth\n",
        "    y = y + tilt_angle\n",
        "\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y = y.astype(float)\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = DEDataset(X, y)\n",
        "trainloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
        "\n",
        "best_loss = 100.0\n",
        "# Run the training loop\n",
        "for epoch in range(0, 15000): # 5 epochs at maximum\n",
        "\n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader):\n",
        "\n",
        "    # Get and prepare inputs\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.double(), targets.double()\n",
        "        targets = targets.reshape((targets.shape[0], 2))\n",
        "\n",
        "        # Perform forward pass\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        # Perform backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform optimization\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        \n",
        "    if epoch % 100 == 0:\n",
        "        cur_loss = (current_loss / (100 * len(trainloader)))\n",
        "        print('Loss after epoch %5d: %.12f' % (epoch, cur_loss))\n",
        "        if best_loss > cur_loss:\n",
        "            best_loss = cur_loss\n",
        "            torch.save(mlp.state_dict(), './train_' + str(exp_number)+ '/best_checkpoint.bin')\n",
        "\n",
        "    current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmgBBzK4-qOk",
        "outputId": "d712c8af-6321-4981-f4ab-75600c7e576a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch     0: 0.017097230884\n",
            "Loss after epoch   100: 0.004477044289\n",
            "Loss after epoch   200: 0.003150767880\n",
            "Loss after epoch   300: 0.002124932224\n",
            "Loss after epoch   400: 0.001539843420\n",
            "Loss after epoch   500: 0.001802785576\n",
            "Loss after epoch   600: 0.001263392926\n",
            "Loss after epoch   700: 0.000990343782\n",
            "Loss after epoch   800: 0.001076941149\n",
            "Loss after epoch   900: 0.000824447324\n",
            "Loss after epoch  1000: 0.000782863774\n",
            "Loss after epoch  1100: 0.000742039187\n",
            "Loss after epoch  1200: 0.000756213441\n",
            "Loss after epoch  1300: 0.000678497958\n",
            "Loss after epoch  1400: 0.000509404041\n",
            "Loss after epoch  1500: 0.000704871966\n",
            "Loss after epoch  1600: 0.000466985961\n",
            "Loss after epoch  1700: 0.000314606698\n",
            "Loss after epoch  1800: 0.000584981609\n",
            "Loss after epoch  1900: 0.000460089320\n",
            "Loss after epoch  2000: 0.000638536283\n",
            "Loss after epoch  2100: 0.000432042987\n",
            "Loss after epoch  2200: 0.000368660332\n",
            "Loss after epoch  2300: 0.000481917680\n",
            "Loss after epoch  2400: 0.000451512116\n",
            "Loss after epoch  2500: 0.000329305953\n",
            "Loss after epoch  2600: 0.000393705030\n",
            "Loss after epoch  2700: 0.000655261546\n",
            "Loss after epoch  2800: 0.000425333959\n",
            "Loss after epoch  2900: 0.000262536546\n",
            "Loss after epoch  3000: 0.000404180757\n",
            "Loss after epoch  3100: 0.000168474955\n",
            "Loss after epoch  3200: 0.000313499697\n",
            "Loss after epoch  3300: 0.000336681720\n",
            "Loss after epoch  3400: 0.000377381321\n",
            "Loss after epoch  3500: 0.000306824777\n",
            "Loss after epoch  3600: 0.000340837654\n",
            "Loss after epoch  3700: 0.000231003221\n",
            "Loss after epoch  3800: 0.000289273969\n",
            "Loss after epoch  3900: 0.000260829482\n",
            "Loss after epoch  4000: 0.000209159031\n",
            "Loss after epoch  4100: 0.000244981645\n",
            "Loss after epoch  4200: 0.000373396254\n",
            "Loss after epoch  4300: 0.000216575042\n",
            "Loss after epoch  4400: 0.000284519415\n",
            "Loss after epoch  4500: 0.000241942007\n",
            "Loss after epoch  4600: 0.000235276671\n",
            "Loss after epoch  4700: 0.000254833184\n",
            "Loss after epoch  4800: 0.000202640575\n",
            "Loss after epoch  4900: 0.000197566580\n",
            "Loss after epoch  5000: 0.000201034432\n",
            "Loss after epoch  5100: 0.000229647983\n",
            "Loss after epoch  5200: 0.000198860724\n",
            "Loss after epoch  5300: 0.000257086415\n",
            "Loss after epoch  5400: 0.000214255714\n",
            "Loss after epoch  5500: 0.000199591628\n",
            "Loss after epoch  5600: 0.000297361138\n",
            "Loss after epoch  5700: 0.000223521726\n",
            "Loss after epoch  5800: 0.000110995259\n",
            "Loss after epoch  5900: 0.000197446381\n",
            "Loss after epoch  6000: 0.000141025608\n",
            "Loss after epoch  6100: 0.000196378894\n",
            "Loss after epoch  6200: 0.000233417481\n",
            "Loss after epoch  6300: 0.000226559252\n",
            "Loss after epoch  6400: 0.000226849033\n",
            "Loss after epoch  6500: 0.000158647928\n",
            "Loss after epoch  6600: 0.000216228553\n",
            "Loss after epoch  6700: 0.000221061611\n",
            "Loss after epoch  6800: 0.000133389870\n",
            "Loss after epoch  6900: 0.000150737991\n",
            "Loss after epoch  7000: 0.000194250632\n",
            "Loss after epoch  7100: 0.000163714936\n",
            "Loss after epoch  7200: 0.000177692145\n",
            "Loss after epoch  7300: 0.000254382570\n",
            "Loss after epoch  7400: 0.000167548378\n",
            "Loss after epoch  7500: 0.000168240851\n",
            "Loss after epoch  7600: 0.000267270720\n",
            "Loss after epoch  7700: 0.000142944181\n",
            "Loss after epoch  7800: 0.000148038411\n",
            "Loss after epoch  7900: 0.000256796547\n",
            "Loss after epoch  8000: 0.000137219722\n",
            "Loss after epoch  8100: 0.000134289820\n",
            "Loss after epoch  8200: 0.000275383147\n",
            "Loss after epoch  8300: 0.000186722746\n",
            "Loss after epoch  8400: 0.000133689207\n",
            "Loss after epoch  8500: 0.000194609255\n",
            "Loss after epoch  8600: 0.000187386502\n",
            "Loss after epoch  8700: 0.000181758301\n",
            "Loss after epoch  8800: 0.000179415668\n",
            "Loss after epoch  8900: 0.000146328624\n",
            "Loss after epoch  9000: 0.000124108415\n",
            "Loss after epoch  9100: 0.000149171362\n",
            "Loss after epoch  9200: 0.000147406662\n",
            "Loss after epoch  9300: 0.000141030836\n",
            "Loss after epoch  9400: 0.000143537639\n",
            "Loss after epoch  9500: 0.000152355051\n",
            "Loss after epoch  9600: 0.000107403483\n",
            "Loss after epoch  9700: 0.000144264098\n",
            "Loss after epoch  9800: 0.000142516670\n",
            "Loss after epoch  9900: 0.000148395152\n",
            "Loss after epoch 10000: 0.000176588563\n",
            "Loss after epoch 10100: 0.001654589897\n",
            "Loss after epoch 10200: 0.000107326080\n",
            "Loss after epoch 10300: 0.000121422608\n",
            "Loss after epoch 10400: 0.000095834936\n",
            "Loss after epoch 10500: 0.000140998933\n",
            "Loss after epoch 10600: 0.000184402423\n",
            "Loss after epoch 10700: 0.000125280362\n",
            "Loss after epoch 10800: 0.000115125720\n",
            "Loss after epoch 10900: 0.000447428522\n",
            "Loss after epoch 11000: 0.000168624570\n",
            "Loss after epoch 11100: 0.000112042787\n",
            "Loss after epoch 11200: 0.000103503751\n",
            "Loss after epoch 11300: 0.000174026994\n",
            "Loss after epoch 11400: 0.000139449389\n",
            "Loss after epoch 11500: 0.000640016144\n",
            "Loss after epoch 11600: 0.000092822969\n",
            "Loss after epoch 11700: 0.000125656984\n",
            "Loss after epoch 11800: 0.000123755160\n",
            "Loss after epoch 11900: 0.000141188715\n",
            "Loss after epoch 12000: 0.000817455989\n",
            "Loss after epoch 12100: 0.000162835977\n",
            "Loss after epoch 12200: 0.000144040790\n",
            "Loss after epoch 12300: 0.000116584110\n",
            "Loss after epoch 12400: 0.000145126442\n",
            "Loss after epoch 12500: 0.000117017418\n",
            "Loss after epoch 12600: 0.000118753415\n",
            "Loss after epoch 12700: 0.000169950058\n",
            "Loss after epoch 12800: 0.000210881296\n",
            "Loss after epoch 12900: 0.000134109044\n",
            "Loss after epoch 13000: 0.000115664330\n",
            "Loss after epoch 13100: 0.000132328563\n",
            "Loss after epoch 13200: 0.000095169372\n",
            "Loss after epoch 13300: 0.000176341302\n",
            "Loss after epoch 13400: 0.000134659777\n",
            "Loss after epoch 13500: 0.000164675138\n",
            "Loss after epoch 13600: 0.000114308102\n",
            "Loss after epoch 13700: 0.000098787892\n",
            "Loss after epoch 13800: 0.000094330587\n",
            "Loss after epoch 13900: 0.000183034296\n",
            "Loss after epoch 14000: 0.000105337861\n",
            "Loss after epoch 14100: 0.000099239419\n",
            "Loss after epoch 14200: 0.000092831092\n",
            "Loss after epoch 14300: 0.000095417793\n",
            "Loss after epoch 14400: 0.000195820634\n",
            "Loss after epoch 14500: 0.000118579549\n",
            "Loss after epoch 14600: 0.000102496119\n",
            "Loss after epoch 14700: 0.000113085607\n",
            "Loss after epoch 14800: 0.000111762452\n",
            "Loss after epoch 14900: 0.000115013839\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xls = pd.ExcelFile('./train_' + str(exp_number)+ '/val' + str(val_number)+ '.xlsx')\n",
        "df2 = pd.read_excel(xls, sample_name)\n",
        "\n",
        "sample_count = val_count\n",
        "y = [0 for i in range(sample_count)]\n",
        "X = []\n",
        "\n",
        "for i in range(sample_count):\n",
        "    X.append(df2[i].values)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y = y.astype(float)\n",
        "\n",
        "dataset = DEDataset(X, y)\n",
        "testloader = DataLoader(dataset, batch_size=val_count, shuffle=False)\n",
        "\n",
        "mlp.load_state_dict(torch.load('./train_' + str(exp_number)+ '/best_checkpoint.bin'))\n",
        "mlp.eval()\n",
        "\n",
        "workbook = xlsxwriter.Workbook('train_' + str(exp_number) + '_' + sample_name + '.xlsx')\n",
        "worksheet = workbook.add_worksheet(sample_name)\n",
        "row = 0\n",
        "col = 0\n",
        "\n",
        "for i, data in enumerate(testloader):\n",
        "    # Get and prepare inputs\n",
        "    inputs, targets = data\n",
        "    inputs, targets = inputs.double(), targets.double()\n",
        "    targets = targets.reshape((targets.shape[0], 1))\n",
        "\n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "    print(inputs)\n",
        "    angles = (outputs[:, :1] * 180.0) / math.pi # angle\n",
        "    des = (outputs[:, 1:2]) * 100.0             # DE\n",
        "\n",
        "    for i, tup in enumerate(zip(angles, des)):\n",
        "        print(tup[0].item(), tup[1].item())\n",
        "        worksheet.write(row, col, tup[0].item())\n",
        "        worksheet.write(row, col + 1, tup[1].item())\n",
        "        row += 1\n",
        "\n",
        "workbook.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zbzd_oK_uNs",
        "outputId": "104da1e6-f071-4f0a-b1ca-43a5edee9faf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0593, 0.8839, 0.7276,  ..., 0.4865, 0.7015, 1.0593],\n",
            "        [1.0857, 0.7766, 0.6916,  ..., 0.4349, 0.7770, 1.0857],\n",
            "        [0.9945, 0.7160, 0.7062,  ..., 0.3932, 0.7180, 0.9945],\n",
            "        ...,\n",
            "        [0.6937, 1.0862, 0.6308,  ..., 0.2201, 0.7237, 0.6937],\n",
            "        [0.7006, 1.1319, 0.6226,  ..., 0.2130, 0.7280, 0.7006],\n",
            "        [0.7602, 1.2292, 0.6170,  ..., 0.2164, 0.6756, 0.7602]])\n",
            "148.7681371946241 152.66871698302026\n",
            "149.87833888222343 159.11775485861227\n",
            "149.17435275915884 158.85976373287065\n",
            "148.74481904784233 160.29568312663213\n",
            "149.50540413515358 158.62151380049062\n",
            "149.39510503656572 158.04761197611643\n",
            "148.25749342118422 163.13936431136125\n",
            "147.48560388260864 163.24638431944297\n",
            "147.81827229890942 163.21161276584718\n",
            "147.57626683911195 161.73560506065468\n",
            "147.2842981573963 158.36811359273418\n",
            "146.82067058519408 158.42013546569777\n",
            "145.9749614559384 159.89124079070828\n",
            "144.19218500676266 161.5197844456641\n",
            "145.46201019341206 159.4202431849287\n",
            "144.53629304558288 162.22301033939942\n",
            "144.62079936750553 168.65399754182994\n",
            "145.19198165914995 169.21534877274263\n",
            "144.50337275240489 169.2743423779851\n",
            "143.9527392276285 178.29597913485412\n",
            "144.06425655701392 174.08493561822843\n",
            "144.155186470071 173.47698041036145\n",
            "145.1107739791649 175.20209505736238\n",
            "144.6686966658538 177.0378456414752\n",
            "145.98744218774902 174.4472096116739\n",
            "146.47138382971468 175.91160057083204\n",
            "144.06634841849535 170.21131925041473\n",
            "145.89232223986062 169.6971016199677\n",
            "147.4291514083063 168.0350361021136\n",
            "146.2498810150985 163.9923727454376\n",
            "147.07099006998635 159.07296165430566\n",
            "145.77150037102263 154.9542627118967\n",
            "148.16667467676288 153.29127202364293\n",
            "145.965601538288 152.46876080266415\n",
            "145.80636697934946 157.17160235034854\n",
            "147.12031301406807 153.8172785636891\n",
            "147.100460307081 157.48330082623926\n",
            "149.8186450372443 156.55758590601957\n",
            "148.3511215708094 154.4472608608602\n",
            "147.90282280734303 154.84013966878024\n",
            "149.71458986842805 151.9381028538508\n",
            "149.31008013976808 154.67086776518533\n",
            "148.91793380905236 155.08989500943423\n",
            "148.9931826376908 158.08808121603755\n",
            "150.9147094476769 160.93848250788807\n",
            "149.77014972243435 161.1610195056923\n",
            "149.91699665930284 164.43320554339374\n",
            "149.8802164630921 170.66558197185287\n",
            "151.00722345410543 171.9027688420125\n",
            "150.31902159023818 169.3113792329928\n",
            "149.7746667017196 169.48245896886252\n"
          ]
        }
      ]
    }
  ]
}